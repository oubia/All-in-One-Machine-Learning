{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an integer batch_size, randomly extract a sub-dataset such that from the original dataset. Note that the random sampling at each iteration has to be done without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(f, grad_f, x0, D, kmax,alpha):\n",
    "    X, y = D\n",
    "    N = X.shape[1]\n",
    "    Xhat = np.concatenate((np.ones((1,N)), X), axis=0)\n",
    "    xsol=x0\n",
    "    x=[x0]\n",
    "    i=0\n",
    "    cond = True\n",
    "    while i < kmax :\n",
    "        x.append(xsol)\n",
    "        i+=1\n",
    "    return x\n",
    "def sgd_optimizer(loss, grad_loss, w0, D, alpha, batch_size, n_epochs):\n",
    "    X, y = D # Unpack the data\\n\",\n",
    "    N = X.shape[1]\n",
    "    d = w0.shape[0]\n",
    "    idx = np.arange(0, N)\n",
    " \n",
    " \n",
    "    # Initialization of history vectors\\n\",\n",
    "    w_history = np.zeros((n_epochs, d)) # Save weights at each iteration\\n\",\n",
    "    loss_history = np.zeros((n_epochs, )) # Save loss values at each iteration\\n\",\n",
    "    grad_norm_history = np.zeros((n_epochs, )) # Save gradient norms at each iteration\\n\",\n",
    "\n",
    "    # Initialize weights\\n\",\n",
    "    w = w0 \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[:,idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Initialize a vector that saves the gradient of the loss at each iteration\\n\",\n",
    "        grad_loss_vec = []\n",
    "\n",
    "        for batch_start in range(0, N, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, N)\n",
    "            X_batch = X[:,batch_start:batch_end]\n",
    "            y_batch = y[batch_start:batch_end]\n",
    "            # Compute the gradient of the loss\n",
    "            gradient = grad_loss(w, X_batch, y_batch)\n",
    "\n",
    "            grad_loss_vec.append(np.linalg.norm(gradient, 2))\n",
    "\n",
    "            # Update weights\n",
    "            w = w - alpha* gradient\n",
    "\n",
    "        # Save the updated values\n",
    "        w_history[epoch] = w[:,0]\n",
    "        loss_history[epoch] = loss(w, X, y)\n",
    "        grad_norm_history[epoch] = np.mean(grad_loss_vec)\n",
    "\n",
    "    return w_history, loss_history, grad_norm_history\n",
    " \n",
    " \n",
    " \n",
    "def loss(f, w, D):\n",
    "    X, y = D\n",
    "    y_pred = f(w, X)\n",
    "    return np.mean(np.square(y_pred -y))\n",
    "def grad_l(f, grad_f, w, D):\n",
    "    X, y = D\n",
    "    return np.mean(2 *grad_f(w, X).T * (f(w, X) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given an integer batch_size, randomly extract a sub-dataset \n",
    " such that \n",
    " from the original dataset. Note that the random sampling at each iteration has to be done without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1707) (1, 1707)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('MNIST.mat')\n",
    "X=data[\"X\"]\n",
    "Y=data[\"I\"]\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 333)\n",
      "(333, 1)\n"
     ]
    }
   ],
   "source": [
    "C1=int(input())\n",
    "C2=int(input())\n",
    "idx=(Y[0,:]==C1 )|( Y[0,:]==C2)\n",
    "X=X[:,idx]\n",
    "Y=Y[:,idx]\n",
    "for i in range(Y.shape[1]):\n",
    "    if Y[:,i]==C1:\n",
    "        Y[:,i]=0\n",
    "    else:\n",
    "        Y[:,i]=1\n",
    "d,N=X.shape\n",
    "Y = Y.reshape((N, 1))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_loss_single(w, x, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss for a single sample (x, y).\n",
    "\n",
    "    Example for a simple linear regression model:\n",
    "    f(w, x) = w^T x\n",
    "    Loss = (f(w, x) - y)^2\n",
    "    grad Loss = 2 * (f(w, x) - y) * x\n",
    "\n",
    "    Parameters:\n",
    "    w: parameter vector, shape (d,)\n",
    "    x: single input sample, shape (d,)\n",
    "    y: scalar label\n",
    "\n",
    "    Returns:\n",
    "    grad: shape (d,), gradient of the loss w.r.t. w for this sample\n",
    "    \"\"\"\n",
    "    # Compute prediction\n",
    "    pred = np.dot(w, x)\n",
    "    # Residual\n",
    "    residual = pred - y\n",
    "    # Gradient of loss: d/dw (residual^2) = 2 * residual * x\n",
    "    grad = 2 * residual * x\n",
    "    return grad\n",
    "\n",
    "def grad_loss_batch(w, X_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss on a mini-batch M:\n",
    "    ∇ℓ(θ; M) = (1/|M|) Σ_{i∈M} ∇ℓ(θ; x^i, y^i)\n",
    "\n",
    "    Parameters:\n",
    "    w: parameter vector, shape (d,)\n",
    "    X_batch: batch of inputs, shape (d, batch_size)\n",
    "    y_batch: batch of labels, shape (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "    grad: shape (d,), the average gradient of the loss over the mini-batch\n",
    "    \"\"\"\n",
    "    batch_size = X_batch.shape[1]\n",
    "    # Initialize gradient accumulator\n",
    "    grad_sum = np.zeros_like(w)\n",
    "    \n",
    "    # Compute gradient for each sample and accumulate\n",
    "    for i in range(batch_size):\n",
    "        x_i = X_batch[:, i]\n",
    "        y_i = y_batch[i]\n",
    "        grad_sum += grad_loss_single(w, x_i, y_i)\n",
    "    \n",
    "    # Average over the batch\n",
    "    grad = grad_sum / batch_size\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient on the batch: [ 1.69307136  1.01083614  1.16756809 -1.76544991  0.14306195]\n"
     ]
    }
   ],
   "source": [
    "d = 5\n",
    "batch_size = 10\n",
    "w = np.random.randn(d)\n",
    "X_batch = np.random.randn(d, batch_size)\n",
    "y_batch = np.random.randn(batch_size)\n",
    "\n",
    "# Compute the gradient of the loss on the batch\n",
    "gradient = grad_loss_batch(w, X_batch, y_batch)\n",
    "print(\"Gradient on the batch:\", gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
