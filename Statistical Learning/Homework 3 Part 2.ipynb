{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce02546",
   "metadata": {},
   "source": [
    "# Homework 3 Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b94c3",
   "metadata": {},
   "source": [
    "## Mohammed Oubia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f697c88",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e4479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fda172",
   "metadata": {},
   "source": [
    "• Given a number Nbatch (usually called batch size), randomly extract a subdataset M with |M| =\n",
    "Nbatch from D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d86c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(f, grad_f, x0, D, kmax,alpha):\n",
    "    X, y = D\n",
    "    N = X.shape[1]\n",
    "    Xhat = np.concatenate((np.ones((1,N)), X), axis=0)\n",
    "    xsol=x0\n",
    "    x=[x0]\n",
    "    i=0\n",
    "    cond = True\n",
    "    while i < kmax :\n",
    "        x.append(xsol)\n",
    "        i+=1\n",
    "    return x\n",
    "def sgd_optimizer(loss, grad_loss, w0, D, alpha, batch_size, n_epochs):\n",
    "    X, y = D # Unpack the data\\n\",\n",
    "    N = X.shape[1]\n",
    "    d = w0.shape[0]\n",
    "    idx = np.arange(0, N)\n",
    " \n",
    " \n",
    "    # Initialization of history vectors\\n\",\n",
    "    w_history = np.zeros((n_epochs, d)) # Save weights at each iteration\\n\",\n",
    "    loss_history = np.zeros((n_epochs, )) # Save loss values at each iteration\\n\",\n",
    "    grad_norm_history = np.zeros((n_epochs, )) # Save gradient norms at each iteration\\n\",\n",
    "\n",
    "    # Initialize weights\\n\",\n",
    "    w = w0 \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[:,idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Initialize a vector that saves the gradient of the loss at each iteration\\n\",\n",
    "        grad_loss_vec = []\n",
    "\n",
    "        for batch_start in range(0, N, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, N)\n",
    "            X_batch = X[:,batch_start:batch_end]\n",
    "            y_batch = y[batch_start:batch_end]\n",
    "            # Compute the gradient of the loss\n",
    "            gradient = grad_loss(w, X_batch, y_batch)\n",
    "\n",
    "            grad_loss_vec.append(np.linalg.norm(gradient, 2))\n",
    "\n",
    "            # Update weights\n",
    "            w = w - alpha* gradient\n",
    "\n",
    "        # Save the updated values\n",
    "        w_history[epoch] = w[:,0]\n",
    "        loss_history[epoch] = loss(w, X, y)\n",
    "        grad_norm_history[epoch] = np.mean(grad_loss_vec)\n",
    "\n",
    "    return w_history, loss_history, grad_norm_history\n",
    " \n",
    " \n",
    " \n",
    "def loss(f, w, D):\n",
    "    X, y = D\n",
    "    y_pred = f(w, X)\n",
    "    return np.mean(np.square(y_pred -y))\n",
    "def grad_l(f, grad_f, w, D):\n",
    "    X, y = D\n",
    "    return np.mean(2 *grad_f(w, X).T * (f(w, X) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b87be",
   "metadata": {},
   "source": [
    "• To test the script above, consider the MNIST dataset we used in the previous laboratories, and do the\n",
    "following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581a90be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1707) (1, 1707)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('MNIST.mat')\n",
    "X=data[\"X\"]\n",
    "Y=data[\"I\"]\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82f746",
   "metadata": {},
   "source": [
    "1. From the dataset, select only two digits. It would be great to let the user input the two digits to\n",
    "select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1027a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "(256, 333)\n",
      "(333, 1)\n"
     ]
    }
   ],
   "source": [
    "C1=int(input())\n",
    "C2=int(input())\n",
    "idx=(Y[0,:]==C1 )|( Y[0,:]==C2)\n",
    "X=X[:,idx]\n",
    "Y=Y[:,idx]\n",
    "for i in range(Y.shape[1]):\n",
    "    if Y[:,i]==C1:\n",
    "        Y[:,i]=0\n",
    "    else:\n",
    "        Y[:,i]=1\n",
    "d,N=X.shape\n",
    "Y = Y.reshape((N, 1))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b397255",
   "metadata": {},
   "source": [
    "2. Do the same operation of the previous homework to obtain the training and test set from (X, Y ),\n",
    "selecting the Ntrain you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f6763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, Ntrain):\n",
    "    d, N = X.shape\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:Ntrain]\n",
    "    test_idx = idx[Ntrain:]\n",
    "    Xtrain = X[:, train_idx]\n",
    "    Ytrain = Y[train_idx]\n",
    "    Xtest = X[:, test_idx]\n",
    "    Ytest = Y[test_idx]\n",
    "    return (Xtrain, Ytrain), (Xtest, Ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce31156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 180) (256, 153) (180, 1)\n",
      "256 180\n"
     ]
    }
   ],
   "source": [
    "(Xtrain, Ytrain), (Xtest, Ytest) = split_data(X, Y, 180)\n",
    "print(Xtrain.shape, Xtest.shape,Ytrain.shape)\n",
    "d,N=Xtrain.shape\n",
    "print(d,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3063be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 180\n"
     ]
    }
   ],
   "source": [
    "d,N=Xtrain.shape\n",
    "print(d,N)\n",
    "XTrainhat = np.concatenate((np.ones((N,1)).T, Xtrain), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c157c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 180)\n",
      "[[1.     1.     1.     ... 1.     1.     1.    ]\n",
      " [0.     0.     0.     ... 0.     0.     0.    ]\n",
      " [0.     0.     0.     ... 0.     0.059  0.    ]\n",
      " ...\n",
      " [0.     0.     0.     ... 0.3175 0.     0.    ]\n",
      " [0.     0.     0.     ... 0.     0.     0.    ]\n",
      " [0.     0.     0.     ... 0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(XTrainhat.shape)\n",
    "print(XTrainhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59098a",
   "metadata": {},
   "source": [
    "3. Consider Lecture 11 on my website about the implementation of Logistic Regression for binary\n",
    "classification https://www.evangelistadavide.com/teaching/logistic_regression2024/. It\n",
    "has not been described in class, but it is equivalent to Linear Regression with a slightly modified\n",
    "model. After carefully reading the post, implement a logistic regression classificator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73977bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    " \n",
    "# Compute the value of f\n",
    "def f(w, xhat):\n",
    "    d,N=xhat.shape\n",
    "    out=np.zeros((N,1))\n",
    "    for i in range(len(out)):\n",
    "        out[i]=sigmoid(xhat[:,i].T@w)\n",
    "    return out\n",
    " \n",
    "# Value of the loss\n",
    "def ell(w, Xhat, Y):\n",
    "    d,N=Xhat.shape\n",
    "    return 1/2*N*np.linalg.norm(f(w,Xhat)-Y,2)\n",
    "# Value of the gradient\n",
    "def grad_ell(w, Xhat, Y):\n",
    "    d,N=Xhat.shape\n",
    "    return (Xhat/N) @(sigmoid(Xhat.T@w) *(1-sigmoid(Xhat.T @ w)) * (f(w,Xhat)-Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0af1d",
   "metadata": {},
   "source": [
    "• Test the logistic regression classificator for different digits and different training set dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a758c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 1)\n",
      "(180, 1)\n"
     ]
    }
   ],
   "source": [
    "k,N=XTrainhat.shape\n",
    "w0=np.random.rand(k,1)* 0.01\n",
    "print((XTrainhat @(sigmoid(XTrainhat.T@w0) *(1-sigmoid(XTrainhat.T @ w0)) * (f(w0,XTrainhat)-Ytrain))).shape)\n",
    "print(f(w0,XTrainhat).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9583f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34491991403046557\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(grad_ell(w0,XTrainhat,Ytrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fb0490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_history, loss_history, grad_norm_history=sgd_optimizer(ell, grad_ell, w0, (XTrainhat,Ytrain), 1e-3, 10, 100)\n",
    "def accuracy(w, X, Y, treshold=0.5):\n",
    "    N = X.shape[1]\n",
    "    a = []\n",
    "    cnt = 0\n",
    "    x = np.concatenate((np.ones((1,N)), X), axis=0)\n",
    "    for i in range(N):\n",
    "        a.append(1 if f(w,x[:,i:i+1]) > treshold else 0)\n",
    "        if a[-1] == Y[i,0]:\n",
    "            cnt += 1\n",
    "    print(\"Correct elements guessed:\",cnt)\n",
    "    print(\"Wrong elements guessed:\",N-cnt)\n",
    "    print(\"Accuracy of Model:\",(cnt/N)*100,\" %\")\n",
    "    return cnt\n",
    "def predict(w, X, treshold=0.5):\n",
    "    a=f(w,X)\n",
    "    if a>0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3acfe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 2 and 3 digits the metrics of classifier for traininng are:\n",
      "Correct elements guessed: 173\n",
      "Wrong elements guessed: 7\n",
      "Accuracy of Model: 96.11111111111111  %\n",
      "with 2 and 3 digits the metrics of classifier for test are:\n",
      "Correct elements guessed: 145\n",
      "Wrong elements guessed: 8\n",
      "Accuracy of Model: 94.77124183006535  %\n"
     ]
    }
   ],
   "source": [
    "print(f\"with {C1} and {C2} digits the metrics of classifier for traininng are:\")\n",
    "cnt=accuracy(w_history[-1],Xtrain,Ytrain,0.5)\n",
    "print(f\"with {C1} and {C2} digits the metrics of classifier for test are:\")\n",
    "cnt=accuracy(w_history[-1],Xtest,Ytest,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215484c",
   "metadata": {},
   "source": [
    "• The training procedure will end up with a set of optimal parameters w\n",
    "∗\n",
    ". Compare w\n",
    "∗ when computed\n",
    "with Gradient Descent and Stochastic Gradient Descent, for different digits and different training set\n",
    "dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e48582",
   "metadata": {},
   "source": [
    "with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25e4d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 2 and 3 digits the metrics of classifier for traininng are with GD:\n",
      "Correct elements guessed: 68\n",
      "Wrong elements guessed: 112\n",
      "Accuracy of Model: 37.77777777777778  %\n",
      "with 2 and 3 digits the metrics of classifier for test are with GD:\n",
      "Correct elements guessed: 63\n",
      "Wrong elements guessed: 90\n",
      "Accuracy of Model: 41.17647058823529  %\n"
     ]
    }
   ],
   "source": [
    "x=GD(ell, grad_ell,w0, (XTrainhat,Ytrain), 1000,1e-4)\n",
    "print(f\"with {C1} and {C2} digits the metrics of classifier for traininng are with GD:\")\n",
    "cnt=accuracy(x[-1],Xtrain,Ytrain,0.5)\n",
    "print(f\"with {C1} and {C2} digits the metrics of classifier for test are with GD:\")\n",
    "cnt=accuracy(x[-1],Xtest,Ytest,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313c523",
   "metadata": {},
   "source": [
    "We try with diffrent numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cd46475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1707) (1, 1707)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('MNIST.mat')\n",
    "X=data[\"X\"]\n",
    "Y=data[\"I\"]\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10032532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "9\n",
      "(256, 283)\n",
      "(283, 1)\n"
     ]
    }
   ],
   "source": [
    "C1=int(input())\n",
    "C2=int(input())\n",
    "idx=(Y[0,:]==C1 )|( Y[0,:]==C2)\n",
    "X=X[:,idx]\n",
    "Y=Y[:,idx]\n",
    "for i in range(Y.shape[1]):\n",
    "    if Y[:,i]==C1:\n",
    "        Y[:,i]=0\n",
    "    else:\n",
    "        Y[:,i]=1\n",
    "d,N=X.shape\n",
    "Y = Y.reshape((N, 1))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed4dfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtrain, Ytrain), (Xtest, Ytest) = split_data(X, Y, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbb47115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 180\n"
     ]
    }
   ],
   "source": [
    "d,N=Xtrain.shape\n",
    "print(d,N)\n",
    "XTrainhat = np.concatenate((np.ones((N,1)).T, Xtrain), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d7ab79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,N=XTrainhat.shape\n",
    "w0=np.random.rand(k,1)* 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f54cdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_history, loss_history, grad_norm_history=sgd_optimizer(ell, grad_ell, w0, (XTrainhat,Ytrain), 1e-3, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99e208de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 6 and 9 digits the metrics of classifier for traininng are:\n",
      "Correct elements guessed: 179\n",
      "Wrong elements guessed: 1\n",
      "Accuracy of Model: 99.44444444444444  %\n",
      "with 6 and 9 digits the metrics of classifier for test are:\n",
      "Correct elements guessed: 103\n",
      "Wrong elements guessed: 0\n",
      "Accuracy of Model: 100.0  %\n"
     ]
    }
   ],
   "source": [
    "print(f\"with {C1} and {C2} digits the metrics of classifier for traininng are:\")\n",
    "cnt=accuracy(w_history[-1],Xtrain,Ytrain,0.5)\n",
    "print(f\"with {C1} and {C2} digits the metrics of classifier for test are:\")\n",
    "cnt=accuracy(w_history[-1],Xtest,Ytest,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e033a",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
